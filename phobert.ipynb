{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thanh\\anaconda3\\envs\\ml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL = 'vinai/phobert-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhobertTokenizer(name_or_path='vinai/phobert-base', vocab_size=64000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t64000: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]),\n",
       " tensor([[    0,   218,     8, 27497, 43269,  2106,  2752,     2,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Tôi là nguyễn thanh sơn\"\n",
    "tokens = tokenizer(text, truncation=True, max_length=512, padding='max_length', return_tensors='pt')\n",
    "# tokens =  tokenizer(text)\n",
    "['input_ids', 'token_type_ids', 'attention_mask']\n",
    "tokens['input_ids'].shape, tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thanh\\anaconda3\\envs\\ml\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\thanh\\.cache\\huggingface\\hub\\models--vinai--phobert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(PRETRAINED_MODEL, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight torch.Size([64001, 768])\n",
      "embeddings.position_embeddings.weight torch.Size([258, 768])\n",
      "embeddings.token_type_embeddings.weight torch.Size([1, 768])\n",
      "embeddings.LayerNorm.weight torch.Size([768])\n",
      "embeddings.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "pooler.dense.weight torch.Size([768, 768])\n",
      "pooler.dense.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 768])\n",
      "torch.Size([1, 512, 768])\n",
      "torch.Size([1, 512, 768])\n",
      "torch.Size([1, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "for i in range(-4, 0):\n",
    "    print(out.hidden_states[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2316,  0.1384, -0.5352,  ...,  0.1940,  0.1455, -0.1548],\n",
       "         [-0.3679,  0.0680,  0.0205,  ...,  0.2471, -0.6660,  0.8733],\n",
       "         [ 0.2082, -0.1114, -0.2401,  ..., -0.0102,  0.1581,  0.7768],\n",
       "         ...,\n",
       "         [-0.3334,  0.5165, -0.2400,  ..., -0.2496, -0.0594, -0.1479],\n",
       "         [-0.3334,  0.5165, -0.2400,  ..., -0.2496, -0.0594, -0.1479],\n",
       "         [-0.3334,  0.5165, -0.2400,  ..., -0.2496, -0.0594, -0.1479]]]), pooler_output=tensor([[ 3.7112e-01,  1.1471e-01,  1.2884e-02,  1.3787e-02,  1.5821e-01,\n",
       "          8.6572e-02, -1.4413e-01, -4.0309e-02,  6.9361e-02, -3.7724e-02,\n",
       "         -2.2245e-01, -1.4241e-01,  7.5507e-02, -2.1895e-01, -1.2571e-01,\n",
       "          1.2403e-01, -2.9454e-02,  1.8689e-01, -2.1430e-01, -1.9809e-01,\n",
       "          1.7439e-01, -4.3342e-01,  1.4012e-01,  1.6872e-01,  2.0444e-01,\n",
       "         -2.8698e-01, -1.8562e-01,  1.7863e-01, -2.7439e-02,  6.9970e-02,\n",
       "         -9.0277e-02, -1.7900e-02, -1.3791e-01, -7.3803e-02,  2.7833e-02,\n",
       "         -2.2759e-01, -2.4592e-01,  2.2822e-01,  1.4244e-02,  3.9048e-01,\n",
       "         -1.6978e-01,  4.9684e-01, -3.2050e-01, -2.3059e-01,  1.9863e-01,\n",
       "          7.9459e-02,  1.8584e-01,  4.9873e-01, -2.2016e-01, -2.0677e-01,\n",
       "          1.0941e-01, -1.3527e-01,  1.2194e-01,  1.9136e-01,  2.4178e-01,\n",
       "         -1.6129e-01,  3.3267e-03,  4.0768e-01,  8.7668e-02,  1.7909e-01,\n",
       "          3.0383e-01,  2.8594e-01, -4.3644e-01,  1.6456e-01, -1.5079e-01,\n",
       "         -2.3585e-01,  5.2633e-02, -3.8418e-02, -3.3989e-02,  2.9274e-01,\n",
       "          1.2026e-01, -1.4080e-01, -8.9967e-02, -1.1472e-01,  2.2467e-02,\n",
       "          1.0098e-01,  5.1772e-03, -1.2111e-01, -3.2845e-02, -6.5694e-02,\n",
       "         -1.9481e-02,  2.4289e-01, -1.6525e-01,  5.2725e-01, -4.1870e-01,\n",
       "          9.8346e-03, -1.7130e-01, -5.6309e-02, -1.5370e-01,  6.4552e-02,\n",
       "          3.1648e-01, -1.8894e-01,  1.3208e-01, -2.9372e-01, -1.1688e-01,\n",
       "         -2.4769e-01, -2.0509e-01, -2.2091e-01, -1.7106e-01, -3.2802e-02,\n",
       "          1.4529e-01,  8.8494e-02, -3.5392e-01, -5.3124e-01, -7.1546e-02,\n",
       "         -1.8255e-01,  2.0572e-02, -3.3816e-02,  4.3558e-01, -3.1414e-01,\n",
       "         -9.5816e-02, -2.2833e-01,  3.6837e-01,  5.8780e-02, -8.1743e-02,\n",
       "         -1.8034e-01,  1.9671e-01, -9.7411e-02, -2.5516e-01,  1.5806e-01,\n",
       "          5.6883e-02,  1.3400e-01, -1.1761e-01,  1.4829e-01, -4.0857e-01,\n",
       "          4.8497e-01, -8.7718e-02,  2.0700e-01, -9.0065e-02,  1.8268e-01,\n",
       "         -3.7922e-01,  9.6298e-02, -4.2028e-02, -1.4670e-01, -4.1010e-03,\n",
       "          2.2355e-02, -1.3400e-02,  1.5578e-01,  1.8915e-01, -1.6967e-01,\n",
       "         -3.2676e-02,  2.2026e-01, -7.9808e-02, -2.4283e-01,  5.1803e-02,\n",
       "          1.7862e-01, -2.7119e-02, -3.0811e-01,  2.7853e-01,  1.2570e-01,\n",
       "         -2.3345e-01,  1.5359e-01, -1.6740e-01,  3.8979e-01, -3.9829e-01,\n",
       "          6.8428e-02,  1.2175e-01,  2.2861e-01, -1.9291e-01, -2.2205e-01,\n",
       "         -2.3689e-01,  2.3586e-01,  2.6382e-02, -4.4873e-01, -1.1623e-01,\n",
       "         -1.4853e-02,  2.5312e-01,  2.7355e-01, -1.7216e-01, -1.6685e-01,\n",
       "         -5.0834e-02, -1.1383e-01, -1.0009e-01, -7.4118e-02,  1.1304e-02,\n",
       "         -1.6308e-01,  4.2538e-02,  7.9116e-02, -2.3023e-03,  4.4107e-03,\n",
       "          1.2946e-01, -1.0604e-01,  4.1380e-02,  2.5470e-01, -1.4687e-01,\n",
       "         -4.3531e-02,  1.6980e-01, -1.5260e-01,  1.0482e-01,  3.5690e-02,\n",
       "         -4.9728e-02,  1.4144e-01,  7.5488e-02, -1.8306e-01, -1.8440e-01,\n",
       "         -1.2809e-01,  1.4883e-01,  6.2614e-02, -4.7976e-02, -7.7452e-02,\n",
       "         -1.6265e-01,  2.2175e-01,  1.1260e-01,  2.0861e-01,  5.9060e-02,\n",
       "          7.7729e-02,  1.1852e-01,  1.4024e-01, -1.3199e-01,  3.0259e-01,\n",
       "         -1.5706e-01,  2.7633e-01,  4.6703e-02, -1.7126e-01, -1.5042e-01,\n",
       "          2.9986e-01,  2.0560e-01, -1.2208e-01,  7.3443e-02,  2.0442e-01,\n",
       "         -8.2379e-02,  8.1152e-02,  5.9198e-02,  3.6519e-01,  9.1532e-02,\n",
       "          3.2200e-01,  3.2373e-02, -2.5715e-01,  9.7685e-02, -1.2002e-01,\n",
       "         -2.6720e-01,  1.4048e-01, -2.2554e-01, -4.0614e-02, -2.8305e-01,\n",
       "         -5.7449e-02,  1.4544e-01, -2.1184e-01, -1.9333e-01, -6.2837e-02,\n",
       "         -2.3786e-02, -1.4216e-04, -1.1588e-01, -1.5338e-01,  3.8572e-02,\n",
       "         -4.3375e-02,  1.4453e-01, -2.3235e-01, -1.1227e-01, -9.9156e-02,\n",
       "          1.3435e-01,  5.5645e-04, -2.2938e-01,  9.0024e-02,  2.5040e-02,\n",
       "         -8.8729e-02,  4.7330e-03,  4.5056e-01,  1.0792e-01, -8.1864e-02,\n",
       "          8.0077e-02, -3.9405e-01, -2.7068e-02,  1.0982e-02, -2.2915e-01,\n",
       "          4.1774e-02, -4.5274e-02,  3.5911e-01,  6.8984e-02, -6.7857e-02,\n",
       "         -2.2253e-01, -1.4031e-01,  1.3063e-01, -1.2765e-01, -1.3987e-01,\n",
       "          1.3962e-01,  9.9247e-02, -2.9636e-01, -1.4092e-01, -1.6490e-01,\n",
       "          1.0322e-01,  7.8868e-02,  1.8386e-01, -3.8986e-03,  3.3910e-01,\n",
       "          1.0451e-01,  5.8468e-02,  1.0974e-01, -3.4998e-01, -1.6871e-01,\n",
       "         -2.1662e-01, -1.2290e-01, -1.1903e-01,  1.2126e-01,  2.0709e-02,\n",
       "         -7.4747e-02, -3.0619e-01,  2.7474e-01, -3.4725e-02,  1.1788e-03,\n",
       "         -3.7588e-01, -1.3031e-01, -1.4094e-01, -4.6254e-02,  1.9379e-01,\n",
       "         -1.8875e-01, -3.5548e-02,  4.3485e-01, -2.5305e-01, -8.1990e-02,\n",
       "         -7.4235e-02, -2.2986e-02,  3.5133e-01, -2.2716e-01,  9.9605e-02,\n",
       "          3.0026e-01,  3.2235e-01,  2.8380e-01, -3.2238e-01, -9.2702e-02,\n",
       "          1.2573e-01, -1.3145e-01, -2.3478e-01,  3.5246e-01,  1.7062e-01,\n",
       "          1.7727e-01, -2.0496e-01,  3.3272e-01, -3.8072e-01,  1.8091e-01,\n",
       "          1.1478e-01,  1.4263e-01,  2.1784e-01, -2.1167e-02,  1.9153e-01,\n",
       "         -2.4617e-01,  1.6282e-02, -6.3072e-02,  3.9224e-01, -2.7540e-01,\n",
       "         -1.6363e-01, -5.1367e-03,  1.1168e-01, -2.7786e-02, -7.4691e-03,\n",
       "          2.8795e-01,  3.9787e-02,  2.9613e-03,  2.8214e-02, -2.0510e-02,\n",
       "          1.0423e-01, -1.7405e-02, -2.1674e-01, -1.0720e-01, -1.0765e-01,\n",
       "         -5.1956e-02, -2.1447e-01, -5.5688e-02, -7.6284e-02, -3.1092e-01,\n",
       "          4.6504e-02,  3.6385e-01, -7.1035e-02,  9.2057e-03,  3.2657e-01,\n",
       "         -2.9713e-02,  2.0219e-01,  1.5786e-01,  8.7734e-02, -1.7890e-01,\n",
       "          1.2775e-01, -3.3020e-01, -8.9067e-02,  1.3612e-01, -1.4687e-01,\n",
       "          1.4902e-01, -2.5976e-01,  3.2013e-01,  3.1576e-01,  3.6503e-01,\n",
       "         -4.2693e-02,  2.0022e-01,  7.3599e-04, -3.5851e-01, -2.2203e-01,\n",
       "         -1.2437e-01,  2.5536e-01,  1.7331e-01,  6.1447e-02,  5.6227e-01,\n",
       "         -2.6493e-01,  2.7542e-01, -1.4706e-01, -7.8311e-02, -7.8795e-02,\n",
       "         -1.1920e-01, -5.7840e-03,  3.0892e-01, -4.1345e-02,  3.3807e-02,\n",
       "         -2.1072e-01,  1.9146e-01,  9.3726e-02,  3.8928e-02,  2.9155e-01,\n",
       "          2.9475e-01,  3.4159e-01, -3.7779e-01, -2.9796e-01,  3.6071e-01,\n",
       "         -2.4807e-02, -4.0281e-01,  1.9681e-01,  2.8720e-01, -1.2510e-01,\n",
       "          5.1052e-01,  3.9095e-01,  2.2328e-01, -1.7882e-01,  8.9625e-02,\n",
       "         -2.2028e-01, -2.7867e-02,  2.4732e-01, -1.8247e-01,  2.9035e-02,\n",
       "          2.5090e-01,  2.8121e-01,  1.6736e-01,  3.0927e-01,  1.3852e-01,\n",
       "          5.0712e-02,  2.5464e-02, -6.5084e-03,  4.1474e-02, -9.0780e-02,\n",
       "          2.4811e-01, -7.7530e-02,  1.6683e-01,  2.2982e-01,  1.4004e-01,\n",
       "         -1.6852e-01,  1.2757e-01,  2.0508e-01, -8.9050e-02, -1.6369e-01,\n",
       "         -1.5001e-01, -2.9350e-01,  1.5682e-01,  1.5366e-01, -5.7383e-02,\n",
       "          8.3988e-02, -2.8551e-01, -1.0298e-01,  1.3643e-01,  3.4669e-01,\n",
       "         -5.9953e-02, -2.0652e-01,  1.5412e-01,  2.7962e-01, -1.7552e-01,\n",
       "          6.3592e-02,  1.4320e-01,  1.8991e-02, -1.5083e-01,  3.3519e-01,\n",
       "          9.1522e-02, -2.9825e-01,  1.0291e-01, -1.9761e-01,  6.1548e-02,\n",
       "         -2.1235e-01, -2.6705e-01, -1.7040e-02,  1.1795e-01,  3.1936e-01,\n",
       "          1.7128e-01,  7.2129e-03,  1.6800e-01, -3.3404e-02, -3.9389e-01,\n",
       "         -9.4314e-02,  3.7337e-01,  3.8331e-01, -1.3015e-01,  1.8903e-01,\n",
       "          1.3567e-01,  2.5322e-01,  1.2926e-01, -3.0510e-02, -2.4712e-01,\n",
       "         -2.3125e-01,  2.4200e-01,  8.6540e-02, -2.6197e-01, -2.2233e-03,\n",
       "          3.7138e-01,  1.5937e-01, -2.3962e-02,  1.4464e-01,  3.2495e-02,\n",
       "          3.0544e-01,  4.6322e-01, -1.2970e-01,  2.9319e-02,  2.8377e-02,\n",
       "         -2.1163e-01, -3.8454e-01, -8.9508e-02,  8.0929e-02,  2.2560e-01,\n",
       "         -2.1336e-01,  4.1818e-02,  1.1228e-01,  2.2536e-01, -1.5929e-01,\n",
       "         -5.3237e-01,  4.5334e-02,  7.5648e-02, -1.7424e-01, -2.4524e-01,\n",
       "          2.5298e-02, -8.0363e-02,  1.8674e-01,  3.1865e-01,  1.6222e-01,\n",
       "          1.7933e-02, -8.0867e-02, -1.2439e-01,  1.6738e-01, -2.5116e-01,\n",
       "          2.7894e-01,  2.2374e-02, -3.0071e-01, -3.2255e-01, -1.6806e-01,\n",
       "         -3.2839e-02, -6.1858e-02,  8.2575e-02, -4.6639e-01, -3.1973e-01,\n",
       "          2.4083e-02, -1.9216e-01,  1.6043e-01,  8.1412e-02,  1.8966e-01,\n",
       "          4.5073e-01,  1.2022e-01,  4.2201e-01, -3.0242e-02, -9.6697e-02,\n",
       "         -8.3442e-02,  1.5701e-01, -1.3820e-01,  8.2337e-02,  3.9805e-01,\n",
       "          2.4747e-01, -1.5260e-01, -3.0563e-01, -2.1648e-01, -8.3855e-02,\n",
       "          6.7663e-02, -6.0054e-02,  1.0145e-01, -2.8421e-01,  1.9432e-01,\n",
       "          1.8348e-01,  1.8934e-01, -9.1494e-02,  1.0256e-01,  7.2150e-02,\n",
       "          7.9553e-02,  5.4985e-02, -1.0061e-02,  9.3669e-02, -1.1015e-01,\n",
       "         -5.2464e-02,  2.2475e-01,  1.6279e-01, -1.5599e-01,  1.2978e-01,\n",
       "          3.4662e-02, -6.6245e-02,  1.9047e-02,  2.7767e-01, -1.0694e-01,\n",
       "          4.4986e-01,  1.1286e-01,  3.2371e-02,  1.6635e-01,  1.6149e-01,\n",
       "          4.5453e-01, -3.7680e-01, -1.1216e-01, -2.7443e-01,  8.0417e-02,\n",
       "          1.4809e-01, -1.0883e-01, -8.4805e-02,  2.1891e-01, -2.4431e-01,\n",
       "         -1.2762e-02,  1.8528e-01, -2.2307e-01, -3.4725e-01, -8.8856e-02,\n",
       "         -1.6006e-01,  2.7869e-02, -1.7225e-02, -5.9164e-02,  3.1959e-02,\n",
       "          3.0747e-01, -2.6370e-01, -3.1042e-01, -6.1999e-02, -2.4044e-01,\n",
       "          1.8090e-01, -1.3234e-01,  2.1566e-01, -1.6458e-01, -1.2212e-01,\n",
       "         -7.4544e-02,  7.5196e-02,  7.8471e-02, -8.2600e-02,  8.9597e-02,\n",
       "          9.9566e-02,  1.5817e-01,  1.6203e-02, -1.8624e-01,  2.1966e-01,\n",
       "         -4.4975e-02, -6.4225e-02, -4.5996e-02,  1.6396e-01, -7.2041e-02,\n",
       "         -2.3038e-01,  1.8145e-01, -3.9548e-01, -1.4637e-01, -1.5854e-01,\n",
       "          9.4004e-02,  8.1347e-02, -2.4488e-01, -3.8957e-01, -2.3821e-01,\n",
       "         -6.6596e-02, -3.3825e-02,  3.3347e-01,  2.2251e-01, -3.2178e-01,\n",
       "         -1.9859e-02,  1.4959e-01, -1.3999e-01,  3.9976e-02,  4.8292e-01,\n",
       "          2.0651e-02, -1.9416e-02, -2.3518e-02,  5.2473e-02, -1.4528e-01,\n",
       "          3.1519e-02, -3.8882e-02, -8.6320e-02,  6.8045e-02, -1.1235e-01,\n",
       "          6.2865e-02, -2.3812e-01, -4.9238e-02, -3.5584e-01,  8.2535e-02,\n",
       "          8.5117e-02,  1.5809e-01, -2.7899e-02,  3.9823e-02,  8.7374e-02,\n",
       "          1.3926e-01, -6.3322e-02, -2.2300e-01, -7.0657e-02, -1.5473e-01,\n",
       "          2.7514e-01, -3.7346e-01,  4.1360e-01,  1.2623e-01, -6.2287e-02,\n",
       "         -1.7080e-01,  2.4173e-01,  4.1459e-01, -1.4825e-02, -3.2068e-01,\n",
       "         -3.8265e-01, -9.9744e-02,  8.9808e-02, -7.2417e-02, -6.9238e-02,\n",
       "          2.2948e-01,  1.1923e-01, -2.6277e-01, -4.3057e-02, -5.0312e-02,\n",
       "         -6.3500e-02,  3.1741e-01, -1.9693e-01, -9.7385e-02,  2.1292e-01,\n",
       "          1.7528e-01,  2.2450e-02, -1.0357e-01,  2.5595e-01,  5.5067e-02,\n",
       "          3.3322e-01,  3.7687e-02,  2.7377e-01, -1.5661e-01, -3.1071e-01,\n",
       "          2.2588e-02, -3.0835e-01, -6.5068e-02,  8.6979e-02,  4.6983e-02,\n",
       "         -2.7648e-01,  2.2981e-01,  2.6154e-01, -1.4821e-01,  5.0561e-02,\n",
       "          4.5730e-01,  3.5460e-02,  3.9749e-02,  1.8607e-01,  2.3886e-01,\n",
       "          2.3878e-01,  1.3780e-02, -2.5017e-02,  2.7081e-01, -2.0581e-01,\n",
       "          5.5846e-02, -1.6441e-01,  1.4045e-01, -2.2544e-02,  5.5837e-02,\n",
       "          5.9013e-02, -9.6636e-02, -2.1680e-01,  8.6812e-02,  2.1385e-01,\n",
       "          2.9613e-02,  3.6021e-01,  4.6017e-01,  3.1553e-01, -2.0330e-02,\n",
       "         -2.8672e-01,  1.8949e-01, -4.4633e-02,  1.3528e-01, -4.7588e-02,\n",
       "          3.4004e-01, -4.4253e-01, -1.3955e-01,  3.1837e-02,  1.6193e-01,\n",
       "          6.0156e-02, -1.5980e-01,  4.0350e-01, -1.6966e-02,  3.5146e-02,\n",
       "         -4.0291e-01, -4.6071e-02, -2.6861e-01]]), hidden_states=(tensor([[[-0.0389, -0.0405, -0.0714,  ..., -0.1039,  0.0179, -0.0621],\n",
       "         [-0.4251,  0.4520, -0.4997,  ...,  0.2851, -0.2212,  0.4371],\n",
       "         [-0.0470,  0.1840,  0.0010,  ...,  0.3937,  0.1045,  0.2667],\n",
       "         ...,\n",
       "         [ 0.0450, -0.0510,  0.0638,  ...,  0.0203, -0.1106, -0.2768],\n",
       "         [ 0.0450, -0.0510,  0.0638,  ...,  0.0203, -0.1106, -0.2768],\n",
       "         [ 0.0450, -0.0510,  0.0638,  ...,  0.0203, -0.1106, -0.2768]]]), tensor([[[ 0.0103, -0.0417, -0.0275,  ..., -0.0786, -0.0435, -0.0349],\n",
       "         [-0.0897,  0.1537, -0.2905,  ...,  0.0950, -0.2430,  0.3699],\n",
       "         [-0.0480,  0.2430, -0.0478,  ...,  0.3348,  0.1661,  0.2446],\n",
       "         ...,\n",
       "         [-0.1474,  0.1714,  0.1375,  ...,  0.1235, -0.3361,  0.0052],\n",
       "         [-0.1474,  0.1714,  0.1375,  ...,  0.1235, -0.3361,  0.0052],\n",
       "         [-0.1474,  0.1714,  0.1375,  ...,  0.1235, -0.3361,  0.0052]]]), tensor([[[ 0.0663, -0.0527,  0.0397,  ..., -0.1109, -0.0508, -0.2023],\n",
       "         [-0.1411,  0.1969, -0.1886,  ...,  0.2387, -0.0163,  0.3685],\n",
       "         [-0.1914, -0.0760, -0.1302,  ...,  0.4792, -0.1013,  0.3586],\n",
       "         ...,\n",
       "         [-0.0757,  0.1213, -0.3442,  ...,  0.2225, -0.0479, -0.0013],\n",
       "         [-0.0757,  0.1213, -0.3442,  ...,  0.2225, -0.0479, -0.0013],\n",
       "         [-0.0757,  0.1213, -0.3442,  ...,  0.2225, -0.0479, -0.0013]]]), tensor([[[-0.0417,  0.0325, -0.0961,  ..., -0.0294, -0.0156, -0.1841],\n",
       "         [-0.1194, -0.1271, -0.0286,  ..., -0.0747, -0.1512,  0.5787],\n",
       "         [-0.0414,  0.0933, -0.0288,  ...,  0.4209, -0.4555,  0.7582],\n",
       "         ...,\n",
       "         [-0.0517,  0.0963, -0.0161,  ...,  0.1758, -0.1170,  0.0070],\n",
       "         [-0.0517,  0.0963, -0.0161,  ...,  0.1758, -0.1170,  0.0070],\n",
       "         [-0.0517,  0.0963, -0.0161,  ...,  0.1758, -0.1170,  0.0070]]]), tensor([[[ 0.1927, -0.1425,  0.1197,  ..., -0.1571,  0.0845, -0.0474],\n",
       "         [ 0.0757,  0.0341, -0.2107,  ..., -0.1871, -0.2066,  0.4554],\n",
       "         [ 0.3488,  0.1064,  0.0427,  ...,  0.2160, -0.3334,  0.4234],\n",
       "         ...,\n",
       "         [ 0.0922,  0.1967, -0.0352,  ..., -0.0161, -0.3414, -0.0829],\n",
       "         [ 0.0922,  0.1967, -0.0352,  ..., -0.0161, -0.3414, -0.0829],\n",
       "         [ 0.0922,  0.1967, -0.0352,  ..., -0.0161, -0.3414, -0.0829]]]), tensor([[[ 0.0542, -0.0116,  0.0724,  ...,  0.0028,  0.0445, -0.2086],\n",
       "         [-0.0356,  0.0407, -0.0910,  ...,  0.4668, -0.6987,  0.5022],\n",
       "         [ 0.4384,  0.3417, -0.1531,  ...,  0.4042, -0.3671,  0.5145],\n",
       "         ...,\n",
       "         [ 0.0436, -0.1281, -0.0748,  ...,  0.0545, -0.2886, -0.1282],\n",
       "         [ 0.0436, -0.1281, -0.0748,  ...,  0.0545, -0.2886, -0.1282],\n",
       "         [ 0.0436, -0.1281, -0.0748,  ...,  0.0545, -0.2886, -0.1282]]]), tensor([[[ 0.0155,  0.0760, -0.0157,  ..., -0.0206,  0.0138, -0.0841],\n",
       "         [-0.1859, -0.0692,  0.0777,  ...,  0.3191, -0.7293,  0.6776],\n",
       "         [ 0.1736,  0.1959,  0.0157,  ...,  0.2820, -0.4541,  0.5929],\n",
       "         ...,\n",
       "         [ 0.2189,  0.3878, -0.0349,  ...,  0.0059, -0.2733, -0.0248],\n",
       "         [ 0.2189,  0.3878, -0.0349,  ...,  0.0059, -0.2733, -0.0248],\n",
       "         [ 0.2189,  0.3878, -0.0349,  ...,  0.0059, -0.2733, -0.0248]]]), tensor([[[ 0.3957,  0.1260,  0.3897,  ..., -0.2033, -0.2904, -0.1554],\n",
       "         [-0.0055, -0.0789,  0.0026,  ...,  0.3277, -0.5678,  0.7348],\n",
       "         [-0.1526,  0.2027,  0.4002,  ...,  0.5055, -0.2620,  0.7968],\n",
       "         ...,\n",
       "         [ 0.0668,  0.4727, -0.0971,  ..., -0.1306, -0.0091,  0.0935],\n",
       "         [ 0.0668,  0.4727, -0.0971,  ..., -0.1306, -0.0091,  0.0935],\n",
       "         [ 0.0668,  0.4727, -0.0971,  ..., -0.1306, -0.0091,  0.0935]]]), tensor([[[ 0.0130,  0.1538, -0.2166,  ..., -0.0223, -0.4404, -0.0078],\n",
       "         [-0.0549, -0.1812,  0.0391,  ...,  0.1986, -0.5575,  0.4448],\n",
       "         [-0.1260,  0.1362,  0.3085,  ...,  0.4949, -0.2299,  0.6579],\n",
       "         ...,\n",
       "         [-0.3439,  0.1875, -0.4337,  ..., -0.0142, -0.0685,  0.1288],\n",
       "         [-0.3439,  0.1875, -0.4337,  ..., -0.0142, -0.0685,  0.1288],\n",
       "         [-0.3439,  0.1875, -0.4337,  ..., -0.0142, -0.0685,  0.1288]]]), tensor([[[ 0.0504, -0.1912,  0.1505,  ..., -0.1035, -0.0680,  0.2626],\n",
       "         [ 0.0464, -0.0198, -0.1879,  ...,  0.2861, -0.3520,  0.2926],\n",
       "         [-0.2505,  0.0086,  0.1991,  ...,  0.5032, -0.2456,  0.5046],\n",
       "         ...,\n",
       "         [-0.3679, -0.0779,  0.0812,  ...,  0.0974, -0.2121, -0.2136],\n",
       "         [-0.3679, -0.0779,  0.0812,  ...,  0.0974, -0.2121, -0.2136],\n",
       "         [-0.3679, -0.0779,  0.0812,  ...,  0.0974, -0.2121, -0.2136]]]), tensor([[[-0.1808, -0.3033, -0.6125,  ..., -0.1645,  0.1484, -0.0020],\n",
       "         [-0.0907, -0.1441, -0.3544,  ...,  0.1886, -0.3655,  0.5117],\n",
       "         [ 0.1736,  0.0228,  0.2238,  ...,  0.1350, -0.4172,  0.6007],\n",
       "         ...,\n",
       "         [-0.3464, -0.0827,  0.0102,  ...,  0.0060, -0.1318, -0.1285],\n",
       "         [-0.3464, -0.0827,  0.0102,  ...,  0.0060, -0.1318, -0.1285],\n",
       "         [-0.3464, -0.0827,  0.0102,  ...,  0.0060, -0.1318, -0.1285]]]), tensor([[[ 0.0148, -0.0063, -0.0285,  ..., -0.0446,  0.0644, -0.0201],\n",
       "         [-0.0514, -0.0367,  0.1155,  ...,  0.2324, -0.3968,  0.4253],\n",
       "         [ 0.3008,  0.0114, -0.1198,  ...,  0.2027, -0.3310,  0.7791],\n",
       "         ...,\n",
       "         [-0.4571,  0.1836, -0.1487,  ..., -0.1926, -0.1226, -0.3705],\n",
       "         [-0.4571,  0.1836, -0.1487,  ..., -0.1926, -0.1226, -0.3705],\n",
       "         [-0.4571,  0.1836, -0.1487,  ..., -0.1926, -0.1226, -0.3705]]]), tensor([[[ 0.2316,  0.1384, -0.5352,  ...,  0.1940,  0.1455, -0.1548],\n",
       "         [-0.3679,  0.0680,  0.0205,  ...,  0.2471, -0.6660,  0.8733],\n",
       "         [ 0.2082, -0.1114, -0.2401,  ..., -0.0102,  0.1581,  0.7768],\n",
       "         ...,\n",
       "         [-0.3334,  0.5165, -0.2400,  ..., -0.2496, -0.0594, -0.1479],\n",
       "         [-0.3334,  0.5165, -0.2400,  ..., -0.2496, -0.0594, -0.1479],\n",
       "         [-0.3334,  0.5165, -0.2400,  ..., -0.2496, -0.0594, -0.1479]]])), past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
